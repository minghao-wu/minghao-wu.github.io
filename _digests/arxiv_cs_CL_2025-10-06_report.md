---
title: "ArXiv Daily Digest on 2025-10-06"
collection: digests
type: "ArXiv daily digest"
permalink: /digests/arxiv_cs_CL_2025-10-06_report
date: 2025-10-06
location: "Online"
---

<span style="color: red;">**Disclaimer: The content below is generated by a large language model (deepseek-chat) and may not be entirely accurate or complete.**</span>

Today's research highlights significant advances in multi-agent collaboration and model optimization frameworks. We see growing emphasis on Theory-of-Mind capabilities in LLMs, with new benchmarks revealing that first-order reasoning (interpreting others' intent) correlates more strongly with collaborative success than complex second-order reasoning. Meanwhile, novel preference optimization methods address the bias-variance trade-off in aligning large reasoning models, achieving substantial performance gains through gradient stabilization techniques. Additionally, multilingual routing analysis in Mixture-of-Experts architectures uncovers language-processing patterns that enable effective cross-lingual interventions, demonstrating how inference-time expert steering can enhance multilingual generalization across diverse models and tasks.

## TL;DR

**TL;DR: Recent arXiv papers focus on improving AI reasoning and collaboration through novel optimization techniques and multi-agent evaluation.**

**Key Themes:**
1. **Preference Optimization for Reasoning Models:** BVPO addresses gradient variance in Large Reasoning Models by mixing high-variance trace-based gradients with low-variance empty-trace gradients, improving alignment (+7.8 pts on AlpacaEval) while preserving reasoning capabilities.

2. **Multi-Agent Collaboration & Theory-of-Mind:** LLM-Hanabi benchmark reveals first-order ToM (interpreting others' intent) correlates more strongly with collaborative success than second-order reasoning, with Large Reasoning Models outperforming standard LLMs in imperfect-information games.

3. **Multilingual Routing in MoE Models:** Analysis shows Mixture-of-Experts models exhibit language-specific routing in early/late layers but cross-lingual alignment in middle layers. Simple inference-time interventions promoting English-activated experts boost multilingual performance by 1-2% across 15+ languages.

**Insights:** These works demonstrate that (1) explicit bias-variance optimization stabilizes training for complex reasoning tasks, (2) accurate interpretation of partner rationale is more critical than complex recursive reasoning for effective collaboration, and (3) cross-lingual expert sharing in middle layers is key to multilingual generalization in sparse models.

---

# From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models

Authors: Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia

Keywords: Mixture-of-Experts, Multilingual Routing, Cross-lingual Transfer, Sparse Activation, Expert Intervention, Language Universality

Comments: None

Paper link: http://arxiv.org/abs/2510.05095v1

## Abstract

Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.

## Summary

This paper investigates multilingual routing patterns in Mixture-of-Experts (MoE) large language models and demonstrates how inference-time interventions can improve cross-lingual performance. The key contributions include a comprehensive analysis of expert routing dynamics across languages and the development of effective intervention methods that enhance multilingual generalization.

The authors conduct extensive interpretability analysis using parallel multilingual datasets (FLoRes-200) across four MoE models (Qwen3, Phi-3.5-MoE, GPT-OSS-20B, OLMoE). They discover a U-shaped pattern in routing divergence from English: early and late layers show language-specific routing, while middle layers exhibit strong cross-lingual alignment. Crucially, they find a strong correlation between a model's performance in a language and how similarly its tokens are routed to English in these middle layers.

Building on these insights, the paper introduces inference-time interventions that steer router logits to activate task-specific experts (identified from English data) during multilingual evaluation. The methodology includes both soft interventions (adding/subtracting values proportional to logit standard deviation) and hard interventions (force-activation/deactivation). The interventions specifically target middle layers where language-universal representations reside.

The results show consistent improvements across three models and two evaluation tasks (MGSM mathematical reasoning and Global-MMLU medicine subset). Interventions yield 1-2% average gains across 15+ languages, with particularly strong improvements for lower-resource languages. The effectiveness is highly sensitive to target layers, with interventions outside the identified middle layers causing performance degradation. This work demonstrates that improved cross-lingual routing alignment directly enhances multilingual generalization, motivating future methods that promote expert sharing during training.

## Critique

Of course. Here is a critique of the paper "Multilingual Routing in Mixture-of-Experts," covering its strengths, weaknesses, and overall significance.

### Overall Summary

This is a strong, well-executed paper that makes a clear and significant contribution to the understanding of multilingual processing in MoE LLMs. It successfully bridges interpretability research on dense models with the unique architecture of MoEs, providing both compelling correlational evidence and, more importantly, causal evidence through effective interventions.

---

### Strengths

1.  **High Novelty and Timeliness:** The focus on interpreting and intervening in the routing mechanisms of multilingual MoE LLMs is highly novel. While there is existing work on interpreting dense models and some on MoE specialization, the combination with a rigorous multilingual analysis is a significant step forward. The paper effectively builds upon known concepts (the "U-shaped" language representation in dense models) and shows how they manifest in a more complex, sparse architecture.

2.  **Compelling and Multi-faceted Analysis:** The paper doesn't rely on a single metric. It presents a cohesive narrative built on several key findings:
    *   **Finding 1 (U-shaped Routing Divergence):** Clearly demonstrates that middle layers are language-universal, mirroring dense models but in a more modular way.
    *   **Finding 2 (Correlation with Performance):** Provides a powerful, intuitive link between routing similarity to English and task performance in a language.
    *   **Finding 5 (Language-Task Modularity):** This is a critical finding. The clear separation between multilingual-specialized experts and task-specific experts provides a strong foundation for the intervention strategy and reveals the internal modular structure of the model.

3.  **Significant Causal Evidence through Intervention:** The core strength of the paper is moving beyond correlation to causation. The intervention methodology is well-designed, using both soft and hard techniques. The fact that simple inference-time interventions can consistently improve performance across **three different models and two distinct tasks** is a remarkable and convincing result. It strongly supports the authors' central thesis that cross-lingual expert sharing is a key driver of multilingual generalization.

4.  **Clarity of Presentation:** The paper is generally well-written and logically structured. The figures (especially Figures 1 and 2) are excellent and immediately convey the core concepts. The methodology for calculating routing divergence and identifying experts is explained clearly, enhancing reproducibility.

---

### Weaknesses

1.  **Limited Explanation of "Why Middle Layers?":** While the paper expertly documents *that* the middle layers are crucial, it offers less insight into *why* this architectural pattern emerges. A deeper discussion or hypothesis on why the model self-organizes into this U-shape (e.g., early layers for token/script processing, middle for semantic abstraction, late for language-specific generation) would strengthen the interpretability contribution.

2.  **Over-reliance on English as a Pivot:** The entire analytical framework is built on divergence from English. While pragmatic (given the models' training data), it reinforces an Anglocentric view. The paper briefly mentions that using other high-resource languages as a pivot flattens the trends, but a more thorough exploration of this could have provided a more nuanced view of the "language-universal" space—is it truly universal or an "English-induced" semantic space?

3.  **Modest Performance Gains:** While statistically significant and highly consistent, the performance improvements (1-2%) are modest in absolute terms. The paper rightly frames this as impressive given the simplicity of the intervention, but it does raise the question of the practical utility. A discussion on whether these gains could be amplified with more sophisticated training-time methods (as suggested in the conclusion) would be valuable.

4.  **Intervention Search Space and Specificity:** The process of tuning the intervention (target layers, `τ`, `λ`) for each model, while necessary, feels a bit like a "black-box" search. The paper could be more explicit about the sensitivity to these hyperparameters and whether any general principles for selecting them were discovered beyond visual inspection of the divergence graphs.

### Conclusion

This is a high-quality paper that makes a substantial contribution. Its strengths—novelty, a clear narrative from analysis to intervention, and significant, reproducible results—far outweigh its weaknesses. The work successfully opens up a new research direction by showing that the routing mechanisms in MoE models are not just an engineering tool for efficiency but are deeply intertwined with and interpretable for core NLP capabilities like multilingualism. It provides a compelling motivation for future work on training methods explicitly designed to enhance cross-lingual expert sharing.

---

# LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game

Authors: Fangzhou Liang, Tianshi Zheng, Chunkit Chan, Yauwai Yim, Yangqiu Song

Keywords: Multi-Agent Collaboration, Theory-of-Mind, Rationale Inference, Imperfect Information Games, Large Language Models, Hanabi Benchmark

Comments: EMNLP 2025 Wordplay

Paper link: http://arxiv.org/abs/2510.04980v1

## Abstract

Effective multi-agent collaboration requires agents to infer the rationale behind others' actions, a capability rooted in Theory-of-Mind (ToM). While recent Large Language Models (LLMs) excel at logical inference, their ability to infer rationale in dynamic, collaborative settings remains under-explored. This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework features an automated evaluation system that measures both game performance and ToM proficiency. Across a range of models, we find a significant positive correlation between ToM and in-game success. Notably, first-order ToM (interpreting others' intent) correlates more strongly with performance than second-order ToM (predicting others' interpretations). These findings highlight that for effective AI collaboration, the ability to accurately interpret a partner's rationale is more critical than higher-order reasoning. We conclude that prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future models.

## Summary

This paper introduces **LLM-Hanabi**, a novel benchmark for evaluating **Theory-of-Mind (ToM)** and **rationale inference** in multi-agent collaboration using the cooperative card game Hanabi. The key contribution is a framework that moves beyond static, text-based ToM evaluations by providing a dynamic, imperfect-information environment where agents must interpret sparse linguistic hints to succeed. The benchmark includes an automated evaluation system that measures both game performance and ToM proficiency through structured reasoning extraction and LLM-as-a-judge scoring.

The methodology leverages Hanabi's cooperative nature, where players cannot see their own cards and must rely on interpreting hints from teammates. During gameplay, agents generate structured statements capturing their reasoning: **rationale** (hinter's intent), **first-order ToM** (recipient's interpretation of intent), and **second-order ToM** (hinter's prediction of recipient's interpretation). These are scored post-game to quantify ToM capabilities, providing a scalable way to assess collaborative reasoning.

Results across diverse LLMs and Large Reasoning Models (LRMs) reveal two major findings: (1) **LRMs significantly outperform standard LLMs** in both game success and ToM performance, with models like Deepseek-R1 and GPT-4.1 achieving the highest scores; (2) **First-order ToM strongly correlates with game performance** (r=0.76), more so than second-order ToM (r=0.58), indicating that accurately interpreting a partner's rationale is more critical for collaboration than predicting how they will interpret hints. These insights suggest that enhancing first-order ToM capabilities should be prioritized for developing more effective collaborative AI systems.

## Critique

### Strengths

1. **Novelty of Approach**: The paper introduces LLM-Hanabi, a novel benchmark that leverages the cooperative card game Hanabi to evaluate Theory-of-Mind (ToM) and rationale inference in LLMs. Using Hanabi is particularly clever, as its mechanics—partial observability and reliance on sparse linguistic hints—naturally isolate the challenges of collaborative reasoning under uncertainty. The automated ToM evaluation system, which extracts first-order and second-order reasoning during gameplay and scores them post-game, is also innovative and scalable.

2. **Significance of Results**: The findings are insightful and directly relevant to the development of collaborative AI. The strong positive correlation between ToM and game performance underscores the importance of rationale inference in multi-agent settings. More notably, the discovery that first-order ToM (interpreting others' intent) correlates more strongly with success than second-order ToM (predicting others' interpretations) provides a clear, actionable direction for future model development. The benchmark also highlights the superior performance of large reasoning models (LRMs) over standard LLMs, emphasizing the value of specialized reasoning architectures.

3. **Clarity of Presentation**: The paper is well-structured, with a logical flow from introduction to evaluation. The methodology is clearly explained, including the design rationale for using Hanabi and the detailed breakdown of the ToM evaluation system. Tables and figures (e.g., Table 1, Figure 2) effectively summarize results and support the analysis. The inclusion of limitations and an ethics statement further strengthens the paper's transparency and rigor.

### Weaknesses

1. **Limited Generalizability**: While Hanabi is an excellent testbed, the benchmark's reliance on a single game environment raises questions about its broader applicability. Real-world collaboration often involves more dynamic, open-ended, and multimodal interactions, which Hanabi does not fully capture. The authors acknowledge this limitation but could have proposed concrete steps for extending the benchmark to other domains.

2. **Potential Bias in Evaluation**: The use of an "LLM-as-a-judge" for ToM scoring introduces subjectivity, as the judge model's own reasoning biases may influence the results. While practical, this approach lacks ground-truth validation (e.g., human evaluation), which could undermine the reliability of the ToM scores. A comparative analysis with human judgments or alternative evaluation methods would have strengthened the claims.

3. **Superficial Analysis of Model Differences**: The paper identifies performance gaps between LLMs and LRMs but does not deeply investigate the underlying reasons. For example, why do LRMs excel in both gameplay and ToM? Are their architectural differences (e.g., longer context, reasoning-specific training) the primary drivers? A more detailed ablation or qualitative analysis could have provided deeper insights.

4. **Narrow Scope of ToM Evaluation**: The benchmark focuses exclusively on hint-based interactions in Hanabi, which, while useful, may not fully represent the spectrum of ToM capabilities (e.g., empathy, deception, or long-term belief tracking). Expanding the evaluation to include diverse collaborative scenarios would make the benchmark more comprehensive.

### Overall Assessment

This paper makes a valuable contribution to the field by proposing a scalable, automated benchmark for evaluating ToM and rationale inference in collaborative AI. The results are significant, particularly the emphasis on first-order ToM as a critical factor for success. However, the narrow scope of the environment and potential evaluation biases limit the generalizability of the findings. Future work could address these issues by extending the benchmark to other games or real-world tasks and incorporating human validation for ToM scoring.

---

# Multilingual Routing in Mixture-of-Experts

Authors: Lucas Bandarkar, Chenyuan Yang, Mohsen Fayyaz, Junlin Hu, Nanyun Peng

Keywords: Large Reasoning Models, Preference Optimization, Bias-Variance Trade-off, Gradient Estimation, Direct Preference Optimization, Trace Sampling, Stochastic Gradient Descent

Comments: None

Paper link: http://arxiv.org/abs/2510.04694v1

## Abstract

Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.

## Summary

Based on the provided paper, here is a summary of its key contributions, methods, and results:

**Key Contributions:**
This paper addresses the challenge of aligning Large Reasoning Models (LRMs) with human preferences, a critical step before deployment that remains underexplored. The authors identify that standard preference optimization methods (like DPO) suffer from high gradient variance when applied to LRMs due to stochastic sampling of reasoning traces. Their main contribution is proposing Bias–Variance Optimized Preference Optimization (BVPO), a simple drop-in method that optimizes the bias-variance trade-off by combining two gradient estimators.

**Methods:**
The paper frames preference optimization for LRMs through the bias-variance trade-off lens. BVPO mixes two gradient estimators:
1. A high-variance trace-based estimator (gt) that uses sampled reasoning traces
2. A low-variance empty-trace estimator (ge) obtained by disabling reasoning trace generation

The combined estimator gc(α) = αgt + (1-α)ge is designed to minimize Mean Squared Error relative to the ideal marginal gradient. The authors provide theoretical guarantees showing that BVPO strictly reduces trace-induced variance, offers a closed-form optimal mixing weight, and tightens SGD convergence bounds under standard conditions.

**Results:**
Extensive experiments on three LRMs demonstrate that BVPO consistently outperforms baselines (DPO and SimPO):
- Improves alignment by up to 7.8 points on AlpacaEval 2 and 6.8 points on Arena-Hard
- Despite being trained only on general conversational data, BVPO also boosts reasoning performance by up to 4.0 points average across six math reasoning benchmarks
- The method preserves and even enhances reasoning capabilities while improving alignment

These results identify trace sampling variance as a key bottleneck in LRM alignment and demonstrate that explicit bias-variance optimization yields more stable training and stronger overall performance.

## Critique

Of course. Here is a commentary on the strengths and weaknesses of the paper "From Noisy Traces to Stable Gradients: Bias–Variance Optimized Preference Optimization for Aligning Large Reasoning Models."

### Strengths

1.  **High Novelty and Timeliness:** The paper identifies and tackles a very specific and underexplored problem: the high gradient variance in aligning Large Reasoning Models (LRMs) due to stochastic trace sampling. While preference optimization (like DPO) is a mature field for standard LLMs, its application to LRMs presents a unique, non-trivial challenge that the authors address with a novel solution. The work is highly relevant given the recent prominence of reasoning models like DeepSeek R1 and GPT-o1.

2.  **Elegant and Principled Approach:** The core idea of BVPO—mixing a high-variance trace-based gradient with a low-variance, deterministic "empty-trace" gradient—is simple, elegant, and well-motivated. Framing the problem through the lens of the bias-variance trade-off and Mean Squared Error (MSE) minimization provides a strong theoretical foundation. The method is presented as a "drop-in" improvement, which enhances its practical appeal.

3.  **Strong and Comprehensive Theoretical Analysis:** The paper is exceptionally strong on theory. It doesn't just propose a heuristic; it provides rigorous proofs for:
    *   Variance reduction (Theorem 1).
    *   MSE-optimal combination of estimators (Theorem 2), with a guarantee that the combined estimator is never worse than the best individual one.
    *   A direct link between the statistical optimality (MSE) and algorithmic performance, showing tighter convergence bounds for SGD (Theorems 3 & 4).
    This theoretical depth significantly strengthens the paper's contributions.

4.  **Compelling and Multi-faceted Empirical Results:** The experimental validation is thorough and convincing. The paper demonstrates improvements not on one, but two key alignment benchmarks (Arena-Hard and AlpacaEval 2), with significant gains (up to +7.8 points). Crucially, it also shows that alignment with general conversational data does not harm—and can even improve—reasoning performance on six distinct math benchmarks. This addresses a critical concern about catastrophic forgetting of core capabilities post-alignment.

5.  **Clarity of Presentation:** The paper is generally well-written and structured. The problem is clearly motivated, the method is explained step-by-step, and the theoretical and empirical sections are logically separated. The use of definitions and a clear narrative helps guide the reader through the complex material.

### Weaknesses

1.  **Limited Exploration of the Mixing Coefficient (α):** While the theory provides a closed-form solution for the optimal α, the practical implementation section is vague on how α was actually set in the experiments. Was the theoretical α* used? Was it tuned as a hyperparameter? If it was tuned, what were the typical optimal values found? A discussion or ablation on the sensitivity and practical selection of α would have been very valuable.

2.  **Agnosticism to Preference Optimization Algorithm:** The authors state that BVPO is agnostic to the underlying preference optimization algorithm (e.g., DPO, SimPO) but only present results using the DPO objective. To fully substantiate this claim, it would be stronger to show that BVPO also provides gains when integrated with other algorithms like SimPO or KTO, rather than just comparing against them as baselines.

3.  **Potential Overstatement of "No Cost" for Reasoning:** The results showing improved reasoning are impressive. However, the conclusion that BVPO "enhances reasoning capabilities" might be slightly overstated. The training data (UltraFeedback) is general-purpose, so the mechanism for this improvement is not entirely clear. It's possible that the improved alignment and stability simply allow the model's inherent reasoning capacity to be expressed more reliably, rather than actively enhancing the underlying reasoning machinery. A more cautious interpretation would be that BVPO effectively preserves and unlocks reasoning performance during alignment.

4.  **Baseline Detail and Reproducibility:** The paper could provide more details on the baseline implementations (DPO, SimPO) for the LRM setting. For instance, did the baseline DPO also use a single sampled trace, as is the common practice the paper critiques? Making this explicit would clarify the exact experimental setup and strengthen the comparison.

### Overall Assessment

This is a high-quality paper that makes a significant contribution to the field of aligning large generative models. It identifies a clear and important problem, proposes a simple yet theoretically grounded solution, and backs it up with extensive theoretical analysis and compelling empirical evidence across multiple models and benchmarks. The weaknesses are relatively minor and primarily concern details of implementation and presentation that could be addressed in a revision. The core ideas are novel, impactful, and likely to influence subsequent work on reasoning model alignment.

